{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Perceptron.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6BTWrldbtZgw"},"source":["# **El Perceptrón**"]},{"cell_type":"markdown","metadata":{"id":"4H0pglG2teFa"},"source":["https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/\n","\n","¿Cómo ocurre la propagacion inversa por medio del gradiente descendiente? Revisar el link\n","\n","Sugerencia: primero leer y entender la parte de la definición general (if __name__ == \"__main__\":) y despues entrar en el modelo del perceprtron (class NeuralNetwork())\n","\n","Ph.D. (c) Manuel Vélez - 2020"]},{"cell_type":"code","metadata":{"id":"2qZxAwNYtzqN","executionInfo":{"status":"ok","timestamp":1659321059324,"user_tz":300,"elapsed":203,"user":{"displayName":"CAMILO ANDRES LOPEZ SARMIENTO","userId":"14537470450628573491"}}},"source":["#Librerías\n","from numpy import exp, array, random, dot\n","#Definición de Variables globales para la funcion de los diferentes procesos que se requieren\n","alfa=0;\n","steps=0;\n","errorTotal=[]\n","StepsCont=0\n","StepsFull=[]"],"execution_count":19,"outputs":[]},{"cell_type":"code","source":["print (\"EL PERCEPTRON\")\n","alfa=int(input(\"Ingrese el valor del alfa: \"))\n","steps=int(input(\"Ingrese el valor de pasos: \"))\n","print (\"Cúal de los modelos desea elegir\")\n","print (\"1. Sigmoidea\")\n","print (\"2. Tangente Hiperbolica\")\n","opcion=int(input(\"Opción: \"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eiJcKLHz1N06","executionInfo":{"status":"ok","timestamp":1659321065781,"user_tz":300,"elapsed":6172,"user":{"displayName":"CAMILO ANDRES LOPEZ SARMIENTO","userId":"14537470450628573491"}},"outputId":"dee32d7c-5a44-460e-b639-cb8d4170ded7"},"execution_count":20,"outputs":[{"name":"stdout","output_type":"stream","text":["EL PERCEPTRON\n","Ingrese el valor del alfa: 10\n","Ingrese el valor de pasos: 100\n","Cúal de los modelos desea elegir\n","1. Sigmoidea\n","2. Tangente Hiperbolica\n","Opción: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ekws2kzIwhUf"},"source":["###Definición del Perceptrón"]},{"cell_type":"code","metadata":{"id":"i9vMFnGmt8ao","executionInfo":{"status":"ok","timestamp":1659321065782,"user_tz":300,"elapsed":29,"user":{"displayName":"CAMILO ANDRES LOPEZ SARMIENTO","userId":"14537470450628573491"}}},"source":["#Perceptrón\n","class NeuralNetwork():\n","    def __init__(self):\n","        # Inicializar la semilla del generador aleatorio para que siempre de los mismos numeros\n","        # cada vez que el programa corra\n","        random.seed(10)\n","\n","        # Modelo de una sola neurona, con una conexion de salida y tres de entrada.\n","        # asignamos pesos aleatorios a una matriz 3 x 1, con los valores en el rango -1 a 1\n","        # y media 0.\n","        self.synaptic_weights = 2 * random.random((3, 1)) - 1\n","\n","    if (opcion == 1):\n","            # La función sigmoidea, que describe una función en forma de s, es la función de activación.\n","            # Nosotros hacemos pasar la suma de los pesos a través de dicha función para normalizarla entre 0 y 1 (porque eso es lo que esperamos) y así poder dar un resultado\n","            def __sigmoid(self, x):\n","                return 1 / (1 + exp(-x))\n","\n","            # La derivada de la función sigmoidea\n","            # es el gradiente descendiente de la función sigmoidea\n","            # Indica qué tanto \"le creemos\" a los pesos resultantes, revisar link.\n","            def __sigmoid_derivative(self, x):\n","                return x * (1 - x)\n","              # Entrenamos a la red neuronal a través de un proceso de prueba y error\n","            # Ajustamos los pesos sinápticos en cada iteración\n","            def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n","                for iteration in range(number_of_training_iterations):\n","                    # Pasamos el conjunto de entrenamiento a través de la red neuronal (una única neurona).\n","                    output = self.think(training_set_inputs)\n","\n","                    # Calculamos el error (La diferencia entre el valor que esperamos obtener realmente\n","                    # y la salida predicha).\n","                    error = training_set_outputs - output\n","                    errorTotal.append(error[0])\n","\n","                    global StepsCont\n","                    StepsCont+=1\n","                    StepsFull.append(StepsCont)\n","\n","\n","                    # Multiplique el error por la entrada y de nuevo por el gradiente descendiente de la función sigmoidea.\n","                    # Esto significa que los pesos menos confiables se ajustan más (filtrado) \n","                    # Esto significa que las entradas, que son cero, no causan cambio a los pesos.\n","                    adjustment = dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n","\n","                    # Ajustar los pesos.\n","                    self.synaptic_weights += (adjustment*alfa)\n","\n","            #Proceso de aprendizaje de la red neuronal:\n","            def think(self, inputs):\n","                # Pasamos las entradas a través de la red neuronal (una única neurona).\n","                return self.__sigmoid(dot(inputs, self.synaptic_weights)) #dot = producto punto\n","\n","    elif (opcion == 2):\n","\n","      # La función Tangente hiperbolica, es la función de activación.\n","      # Nosotros hacemos pasar la suma de los pesos a través de dicha función para normalizarla entre 0 y 1 (porque eso es lo que esperamos) y así poder dar un resultado\n","      def __tangenteh(self, x):\n","          return (2/(1+exp(-2*x)))-1  \n","          \n","      # La derivada de la función Tangente hiperbolica\n","      # es el gradiente descendiente de la función Tangente hiperbolica\n","      # Indica qué tanto \"le creemos\" a los pesos resultantes, revisar link.\n","      def __tangenteh_derivative(self, x):\n","          return (4*exp(-2*x))/((exp(-2*x))+1)**2\n","        # Entrenamos a la red neuronal a través de un proceso de prueba y error\n","\n","      # Ajustamos los pesos sinápticos en cada iteración\n","      def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n","          for iteration in range(number_of_training_iterations):\n","              # Pasamos el conjunto de entrenamiento a través de la red neuronal (una única neurona).\n","              output = self.think(training_set_inputs)\n","\n","              # Calculamos el error (La diferencia entre el valor que esperamos obtener realmente\n","              # y la salida predicha).\n","              error = training_set_outputs - output\n","              errorTotal.append(error[0])\n","\n","              global StepsCont\n","              StepsCont+=1\n","              StepsFull.append(StepsCont)\n","\n","              # Multiplique el error por la entrada y de nuevo por el gradiente descendiente de la función Tangente hiperbolica.\n","              # Esto significa que los pesos menos confiables se ajustan más (filtrado) \n","              # Esto significa que las entradas, que son cero, no causan cambio a los pesos.\n","              adjustment = dot(training_set_inputs.T, error * self.__tangenteh_derivative(output))\n","\n","              # Ajustar los pesos.\n","              self.synaptic_weights += (adjustment*alfa)\n","\n","      #Proceso de aprendizaje de la red neuronal:\n","      def think(self, inputs):\n","          # Pasamos las entradas a través de la red neuronal (una única neurona).\n","          return self.__tangenteh(dot(inputs, self.synaptic_weights)) #dot = producto punto\n","\n","    else:\n","      print (\"no es una opción\")\n","\n","\n","  "],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SeESy5syweNb"},"source":["###Clase Principal"]},{"cell_type":"code","metadata":{"id":"O0FY1SeMuCdO","colab":{"base_uri":"https://localhost:8080/","height":491},"executionInfo":{"status":"ok","timestamp":1659321066312,"user_tz":300,"elapsed":557,"user":{"displayName":"CAMILO ANDRES LOPEZ SARMIENTO","userId":"14537470450628573491"}},"outputId":"932d14fc-fee0-46de-cdef-3b7af7105360"},"source":["#Clase principal\n","if __name__ == \"__main__\":\n","\n","    #Inicialice una red neuronal de una sola neurona. Quizas no sea propiamente una red... \n","    neural_network = NeuralNetwork()\n","\n","    print (\"Pesos sinapticos iniciales generados aleatoriamente: \")\n","    print (neural_network.synaptic_weights)\n","\n","    # El conjunto de entrenamiento. Tenemos 4 ejemplos, cada uno consistente de tres valores de entrada con su respectiva salida (una salida)\n","    training_set_inputs = array([[0, 0, 1], [1, 1, 1], [1, 0, 1], [0, 1, 1]])\n","    training_set_outputs = array([[0, 1, 1, 0]]).T\n","\n","    # Entrene a la red neuronal usando un conjunto de entrenamiento.\n","    # lo iteramos 10,000 veces, haciendo pequeños ajustes de pesos en cada iteración\n","    neural_network.train(training_set_inputs, training_set_outputs, steps)\n","\n","    print (\"Nuevos pesos sinapticos después del entremaniento: \")\n","    print (neural_network.synaptic_weights)\n","\n","    # Pruebe la red neuronal con una situacion desconocida.\n","    test = array([1, 0, 0])\n","    print (\"Considerando las entradas: \")\n","    print (test)\n","    print (\"---------------------------\")\n","    print (neural_network.think(test))\n","    resultado = neural_network.think(test)\n","    if (resultado <= 0.499):\n","        resultado = 0\n","    else:\n","        resultado = 1;\n","    print (resultado)\n","\n","#grafica\n","import matplotlib.pyplot as plt\n","for i in range(len(errorTotal)):\n","  plt.scatter(StepsFull[i],(errorTotal[i]*-1),color=\"blue\")"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Pesos sinapticos iniciales generados aleatoriamente: \n","[[ 0.54264129]\n"," [-0.9584961 ]\n"," [ 0.26729647]]\n","Nuevos pesos sinapticos después del entremaniento: \n","[[13.94618875]\n"," [ 2.36487573]\n"," [ 7.79649042]]\n","Considerando las entradas: \n","[1 0 0]\n","---------------------------\n","[1.]\n","1\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"mm3uEKGPxtXQ"},"source":["**Tarea**: \n","\n","1.   Agregue a este código la función de normalización para que produzca una salida discreta.\n","2.   Agregue la tasa de aprendizaje (denotada como alfa), recuerde que es un factor multiplicativo a la hora de corregir los pesos sinápticos con el error determinado\n","3.  Cree una variable aparte que permita modificar el número de épocas.\n","4.  Agregue una gráfica el cual me permita ver la disminución del error (o la cantidad de aprendizaje) vs el número de épocas. (Como en el ejemplo de funciones de activación)."]}]}